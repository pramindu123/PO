{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbc9048",
   "metadata": {},
   "source": [
    "# Purchase Order Image Reader\n",
    "\n",
    "This notebook reads purchase order images, extracts data using OpenCV and OCR, and outputs the results to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9bca8",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "424c25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install opencv-python pytesseract pandas numpy pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c697252a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Using Tesseract OCR + OpenCV for image processing\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Set the Tesseract executable path (located in tessaret folder)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'D:\\Year 4 Sem 1\\PO\\PO\\tessaret\\tesseract.exe'\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Using Tesseract OCR + OpenCV for image processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e9152",
   "metadata": {},
   "source": [
    "## 2. Image Preprocessing Functions\n",
    "\n",
    "These functions help improve OCR accuracy by preprocessing the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f1501620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions defined!\n"
     ]
    }
   ],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image using PIL first (supports more formats like webp), then convert to OpenCV format.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "    \n",
    "    Returns:\n",
    "        Image in OpenCV BGR format\n",
    "    \"\"\"\n",
    "    # Use PIL to load the image (supports webp, png, jpg, etc.)\n",
    "    pil_image = Image.open(image_path)\n",
    "    \n",
    "    # Convert to RGB if necessary\n",
    "    if pil_image.mode != 'RGB':\n",
    "        pil_image = pil_image.convert('RGB')\n",
    "    \n",
    "    # Convert PIL image to numpy array (OpenCV format)\n",
    "    img = np.array(pil_image)\n",
    "    \n",
    "    # Convert RGB to BGR (OpenCV uses BGR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocess an image for better OCR results.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image ready for OCR\n",
    "    \"\"\"\n",
    "    # Read the image using PIL-based loader\n",
    "    img = load_image(image_path)\n",
    "    \n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply noise reduction\n",
    "    denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "    \n",
    "    # Apply adaptive thresholding for better text detection\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "    \n",
    "    # Deskew the image if needed\n",
    "    thresh = deskew_image(thresh)\n",
    "    \n",
    "    return thresh\n",
    "\n",
    "\n",
    "def deskew_image(image):\n",
    "    \"\"\"\n",
    "    Deskew an image to straighten text lines.\n",
    "    \"\"\"\n",
    "    coords = np.column_stack(np.where(image > 0))\n",
    "    if len(coords) == 0:\n",
    "        return image\n",
    "    \n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    \n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    \n",
    "    # Only deskew if the angle is significant\n",
    "    if abs(angle) > 0.5:\n",
    "        (h, w) = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        image = cv2.warpAffine(\n",
    "            image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE\n",
    "        )\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def enhance_image(image_path):\n",
    "    \"\"\"\n",
    "    Apply multiple enhancement techniques to improve OCR accuracy.\n",
    "    \"\"\"\n",
    "    # Use PIL-based loader to support webp and other formats\n",
    "    img = load_image(image_path)\n",
    "    \n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    \n",
    "    # Resize image if too small (OCR works better on larger images)\n",
    "    height, width = img.shape[:2]\n",
    "    if width < 1000:\n",
    "        scale = 1000 / width\n",
    "        img = cv2.resize(img, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    \n",
    "    # Apply bilateral filter to reduce noise while keeping edges sharp\n",
    "    filtered = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
    "    \n",
    "    # Apply Otsu's thresholding\n",
    "    _, binary = cv2.threshold(filtered, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "\n",
    "print(\"Preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8836405d",
   "metadata": {},
   "source": [
    "## 3. OCR and Text Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c1b21746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR functions defined!\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_image(image_path, preprocess=True, processed_img=None):\n",
    "    \"\"\"\n",
    "    Extract text from an image using OCR.\n",
    "    Tries multiple PSM modes and combines results for best extraction.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        preprocess: Whether to apply preprocessing\n",
    "        processed_img: Optional preprocessed image to use directly\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text as a string\n",
    "    \"\"\"\n",
    "    if processed_img is not None:\n",
    "        img_for_ocr = processed_img\n",
    "    elif preprocess:\n",
    "        img_for_ocr = enhance_image(image_path)\n",
    "    else:\n",
    "        img_for_ocr = cv2.imread(image_path)\n",
    "    \n",
    "    # Also load original image for alternative OCR attempts\n",
    "    original_img = load_image(image_path)\n",
    "    \n",
    "    # Try PSM 6 first (default for uniform blocks of text) - usually better for structured docs\n",
    "    text_psm6 = pytesseract.image_to_string(img_for_ocr, config='--oem 3 --psm 6')\n",
    "    \n",
    "    # Also try PSM 3 on original image (better for some document types)\n",
    "    text_psm3 = pytesseract.image_to_string(original_img, config='--oem 3 --psm 3')\n",
    "    \n",
    "    # Combine both outputs - PSM 6 (preprocessed) first, then PSM 3\n",
    "    # This ensures structured doc patterns are found first\n",
    "    text = text_psm6 + \"\\n\" + text_psm3\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_text_with_boxes(image_path):\n",
    "    \"\"\"\n",
    "    Extract text with bounding box information.\n",
    "    \"\"\"\n",
    "    processed_img = enhance_image(image_path)\n",
    "    \n",
    "    # Get detailed OCR data\n",
    "    data = pytesseract.image_to_data(processed_img, output_type=pytesseract.Output.DICT)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"OCR functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a0a29",
   "metadata": {},
   "source": [
    "## 4. Purchase Order Data Parsing Functions\n",
    "\n",
    "These functions parse the extracted text to identify common purchase order fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "48fb24a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing functions defined!\n"
     ]
    }
   ],
   "source": [
    "def normalize_date(date_str):\n",
    "    \"\"\"\n",
    "    Normalize various date formats to YYYY-MM-DD format.\n",
    "    \n",
    "    Args:\n",
    "        date_str: Date string in various formats\n",
    "    \n",
    "    Returns:\n",
    "        Date string in YYYY-MM-DD format, or original if parsing fails\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    date_str = date_str.strip()\n",
    "    \n",
    "    # List of possible date formats to try\n",
    "    date_formats = [\n",
    "        # DD/MM/YY or DD-MM-YY\n",
    "        '%d/%m/%y', '%d-%m-%y',\n",
    "        # DD/MM/YYYY or DD-MM-YYYY\n",
    "        '%d/%m/%Y', '%d-%m-%Y',\n",
    "        # MM/DD/YY or MM-DD-YY\n",
    "        '%m/%d/%y', '%m-%d-%y',\n",
    "        # MM/DD/YYYY or MM-DD-YYYY\n",
    "        '%m/%d/%Y', '%m-%d-%Y',\n",
    "        # YY/MM/DD or YY-MM-DD\n",
    "        '%y/%m/%d', '%y-%m-%d',\n",
    "        # YYYY/MM/DD or YYYY-MM-DD\n",
    "        '%Y/%m/%d', '%Y-%m-%d',\n",
    "        # Month name formats\n",
    "        '%b %d, %Y', '%B %d, %Y',  # Jun 22, 2021 or June 22, 2021\n",
    "        '%b %d %Y', '%B %d %Y',    # Jun 22 2021 or June 22 2021\n",
    "        '%d %b %Y', '%d %B %Y',    # 22 Jun 2021 or 22 June 2021\n",
    "        '%d %b, %Y', '%d %B, %Y',  # 22 Jun, 2021\n",
    "    ]\n",
    "    \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(date_str, fmt)\n",
    "            # If year is < 100, assume 2000s for years < 50, 1900s otherwise\n",
    "            if parsed_date.year < 100:\n",
    "                if parsed_date.year < 50:\n",
    "                    parsed_date = parsed_date.replace(year=parsed_date.year + 2000)\n",
    "                else:\n",
    "                    parsed_date = parsed_date.replace(year=parsed_date.year + 1900)\n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # If no format matched, return original\n",
    "    return date_str\n",
    "\n",
    "\n",
    "def parse_purchase_order(text):\n",
    "    \"\"\"\n",
    "    Parse extracted text to identify purchase order fields.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text extracted from OCR\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing parsed PO data\n",
    "    \"\"\"\n",
    "    po_data = {\n",
    "        'po_number': None,\n",
    "        'date': None,\n",
    "        'vendor_name': None,\n",
    "        'vendor_address': None,\n",
    "        'total_amount': None,\n",
    "        'items': [],\n",
    "        'raw_text': text\n",
    "    }\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Patterns for common PO fields\n",
    "    # Added patterns for OCR misreads like POw: (# read as w), PO#, PO:, etc.\n",
    "    # Also includes \"Works Order No:\", \"No.\", \"REQUISITION No\" patterns\n",
    "    po_patterns = [\n",
    "        r'Works?\\s*Order\\s*(?:Number|No\\.?|#)?\\s*[:\\s]*([A-Z0-9-]+)',  # Works Order No: SW02132230W\n",
    "        r'(?:REQUISITION|Requisition)\\s*No[.,]?\\s*([0-9]+)',  # REQUISITION No, 4165\n",
    "        r'No[.,]\\s*([0-9]+)',  # No. 4165 or No, 4165\n",
    "        r'PO[W#:.\\s]+\\s*([0-9]+)',  # Handles PO#, PO:, PO. followed by numbers\n",
    "        r'P\\.?O\\.?\\s*#?\\s*[:\\s]*([0-9]+)',  # P.O.# or PO# followed by numbers\n",
    "        r'P\\.?O\\.?\\s*(?:Number|No\\.?|#)?\\s*[:\\s]*([A-Z0-9-]+)',\n",
    "        r'Purchase\\s*Order\\s*(?:Number|No\\.?|#)?\\s*[:\\s]*([A-Z0-9-]+)',\n",
    "        r'Order\\s*(?:Number|No\\.?|#)?\\s*[:\\s]*([A-Z0-9-]+)',\n",
    "    ]\n",
    "    \n",
    "    date_patterns = [\n",
    "        r'[Dd]ate\\s*[:\\s]*([0-9]{1,2}[/-][0-9]{1,2}[/-][0-9]{2,4})',\n",
    "        r'DATE\\s*[:\\s]*([0-9]{1,2}[/-][0-9]{1,2}[/-][0-9]{2,4})',\n",
    "        r'([0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4})',  # dd/mm/yy or dd/mm/yyyy\n",
    "        r'([0-9]{1,2}-[0-9]{1,2}-[0-9]{2,4})',  # dd-mm-yy\n",
    "        r'([A-Za-z]+\\s+[0-9]{1,2},?\\s+[0-9]{4})',\n",
    "    ]\n",
    "    \n",
    "    amount_patterns = [\n",
    "        r'[Pp]rice\\s*[:\\s]*([0-9,\\s]+)',  # Price: 60 000 or Price 60,000\n",
    "        r'[Ff]\\s*([0-9]{2,3},\\s*[0-9]{3})',  # F 60,000 (OCR might read £ as F)\n",
    "        r'(\\d{2,3},\\s*\\d{3})',  # 60,000 or 60, 000\n",
    "        r'Total\\s*[:\\s]*\\$?([0-9,\\s]+\\.?[0-9]*)',\n",
    "        r'Grand\\s*Total\\s*[:\\s]*\\$?([0-9,\\s]+\\.?[0-9]*)',\n",
    "        r'Amount\\s*Due\\s*[:\\s]*\\$?([0-9,\\s]+\\.?[0-9]*)',\n",
    "        r'SUBTOTAL\\s*[:\\s]*\\$?([0-9,\\s]+\\.?[0-9]*)',\n",
    "        r'\\$\\s*([0-9,]+\\.[0-9]{2})',\n",
    "    ]\n",
    "    \n",
    "    # Patterns for vendor/supplier/customer name\n",
    "    # Handle various formats: next line, same line, with/without colon\n",
    "    vendor_patterns = [\n",
    "        # Pattern for \"Customer: CRYSTAL MARTIN (HONG KONG)LTD [code]\" - extract the company name\n",
    "        r'Customer\\s*:\\s*([A-Z][A-Z\\s]+\\s*\\([A-Z\\s]+\\)\\s*(?:LTD|Ltd))',\n",
    "        # Direct pattern for COMPANY NAME (LOCATION)LTD format - prioritize this for CRYSTAL MARTIN (HONG KONG)LTD\n",
    "        r'([A-Z][A-Z\\s]+\\s*\\([A-Z\\s]+\\)\\s*(?:LTD|Ltd))',\n",
    "        # Pattern to find company name with LTD/Ltd after another company - extract the SECOND one (customer)\n",
    "        r'(?:PVT|Pvt|Private)?\\.?\\s*(?:LTD|Ltd|LLC|Inc|Corp|Co)\\.?\\s*\\n+\\s*([A-Z][A-Za-z0-9\\s&.,\\'\\(\\)-]+(?:LTD|Ltd|LLC|Inc|Corp|Co))',\n",
    "        r'(?:Supplier|Vendor|Customer)\\s*:\\s*\\n+\\s*\\n*\\s*([A-Z][A-Za-z0-9\\s&.,\\'\\(\\)\\[\\]-]+(?:LTD|LLC|Inc|Ltd|Corp|Co|PVT)?)',  # Customer: \\n\\n NAME\n",
    "        r'(?:Supplier|Vendor|Customer)\\s*\\n+\\s*([^\\n#]+)',  # Supplier/Vendor/Customer followed by newline then name\n",
    "        r'(?:Supplier|Vendor|Customer)\\s+(?:PO|P\\.O\\.).*?\\n+\\s*([A-Za-z\\]\\[]+[A-Za-z0-9\\s&.,\\'\\[\\]-]*?(?:LLC|Inc|Ltd|Corp|Co))(?:\\s+Invoice|\\s+Date|\\s*\\n|$)',\n",
    "        r'(?:Supplier|Vendor|Customer|Bill\\s*From|Ship\\s*From|Bill\\s*To|Ship\\s*To)\\s*[:\\s]+([A-Za-z][A-Za-z0-9\\s&.,\\'\\(\\)-]+?(?:LTD|LLC|Inc|Ltd|Corp|Co|PVT)?)',\n",
    "        r'(?:SUPPLIER|VENDOR|CUSTOMER)\\s*:\\s*\\n+\\s*([^\\n]+)',\n",
    "    ]\n",
    "    \n",
    "    # Extract PO Number\n",
    "    for pattern in po_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            po_num = match.group(1).strip()\n",
    "            # Validate: PO number should have at least some digits and not be common words\n",
    "            if re.search(r'\\d', po_num) and po_num.upper() not in ['BOX', 'DATE', 'ORDER']:\n",
    "                # Keep the full PO/Works Order number (including letters like SW02132230W)\n",
    "                po_data['po_number'] = po_num\n",
    "                break\n",
    "    \n",
    "    # Extract Date\n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            raw_date = match.group(1).strip()\n",
    "            po_data['date'] = normalize_date(raw_date)\n",
    "            break\n",
    "    \n",
    "    # Extract Vendor/Supplier Name\n",
    "    for pattern in vendor_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            vendor = match.group(1).strip()\n",
    "            # Clean up vendor name - remove OCR artifacts and trailing punctuation\n",
    "            vendor = re.sub(r'[\\[\\]]', '', vendor)  # Remove OCR brackets\n",
    "            vendor = re.sub(r'[\\s,.:]+$', '', vendor)  # Remove trailing punctuation\n",
    "            vendor = re.sub(r'\\s+', ' ', vendor)  # Normalize spaces\n",
    "            # Only accept if it looks like a company name (has letters)\n",
    "            if len(vendor) > 2 and re.search(r'[A-Za-z]{2,}', vendor):\n",
    "                po_data['vendor_name'] = vendor\n",
    "                break\n",
    "    \n",
    "    # Extract Total Amount\n",
    "    amounts = []\n",
    "    for pattern in amount_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        for m in matches:\n",
    "            try:\n",
    "                # Remove spaces and commas before converting to float (handles \"60 000\")\n",
    "                clean_amount = m.replace(',', '').replace(' ', '')\n",
    "                amount = float(clean_amount)\n",
    "                amounts.append(amount)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if amounts:\n",
    "        po_data['total_amount'] = max(amounts)  # Usually the largest amount is the total\n",
    "    \n",
    "    # Extract line items - improved pattern matching\n",
    "    # Combined pattern: \"1 Desktop furniture 1 $ 232.00 $ 232.00\"\n",
    "    # Format: item_num description qty $rate $amount\n",
    "    combined_item_pattern = r'^(\\d+)\\.?\\s+(.+?)\\s+(\\d+)\\s+\\$\\s*([\\d,]+\\.?\\d*)\\s+\\$\\s*([\\d,]+\\.?\\d*)$'\n",
    "    \n",
    "    # First, try to extract Product Description and Quantity directly (new PO format)\n",
    "    # Pattern for \"Product Description: description text\"\n",
    "    prod_desc_match = re.search(r'Product\\s*Description\\s*:\\s*(.+?)(?:\\n|$)', text, re.IGNORECASE)\n",
    "    # Pattern for \"Quantity: 1319 units\" or just \"Quantity: 1319\"\n",
    "    qty_match = re.search(r'Quantity\\s*:\\s*(\\d+)\\s*(?:units?)?', text, re.IGNORECASE)\n",
    "    \n",
    "    if prod_desc_match and qty_match:\n",
    "        description = prod_desc_match.group(1).strip()\n",
    "        quantity = int(qty_match.group(1))\n",
    "        po_data['items'].append({\n",
    "            'quantity': quantity,\n",
    "            'description': description,\n",
    "            'price': 0,\n",
    "            'amount': 0\n",
    "        })\n",
    "        return po_data\n",
    "    \n",
    "    # Try combined pattern first (description + prices on same line)\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        match = re.match(combined_item_pattern, line)\n",
    "        if match:\n",
    "            item_num = int(match.group(1))\n",
    "            description = match.group(2).strip()\n",
    "            qty = int(match.group(3))\n",
    "            rate = float(match.group(4).replace(',', ''))\n",
    "            amount = float(match.group(5).replace(',', ''))\n",
    "            \n",
    "            # Validate it's a real item\n",
    "            if item_num > 0 and item_num < 100 and qty > 0 and qty < 1000 and rate > 0:\n",
    "                if not any(x in description.lower() for x in ['total', 'subtotal', 'payment', 'balance']):\n",
    "                    po_data['items'].append({\n",
    "                        'quantity': qty,\n",
    "                        'description': description,\n",
    "                        'price': rate,\n",
    "                        'amount': amount\n",
    "                    })\n",
    "    \n",
    "    # If no items found with combined pattern, try separate patterns\n",
    "    if not po_data['items']:\n",
    "        # Pattern for lines with Qty, Rate/Price, Amount format\n",
    "        price_line_pattern = r'^(\\d+)\\s+\\$\\s*([\\d,]+\\.?\\d*)\\s+\\$\\s*([\\d,]+\\.?\\d*)$'\n",
    "        # Pattern for numbered item descriptions\n",
    "        desc_pattern = r'^(\\d+)\\.?\\s+([A-Za-z][A-Za-z\\s]+?)$'\n",
    "        \n",
    "        descriptions = []\n",
    "        price_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            price_match = re.match(price_line_pattern, line)\n",
    "            if price_match:\n",
    "                qty = int(price_match.group(1))\n",
    "                rate = float(price_match.group(2).replace(',', ''))\n",
    "                amount = float(price_match.group(3).replace(',', ''))\n",
    "                if qty > 0 and qty < 100 and rate > 0 and amount > 0:\n",
    "                    price_lines.append({'qty': qty, 'rate': rate, 'amount': amount})\n",
    "                continue\n",
    "            \n",
    "            desc_match = re.match(desc_pattern, line)\n",
    "            if desc_match:\n",
    "                num = int(desc_match.group(1))\n",
    "                desc = desc_match.group(2).strip()\n",
    "                if num > 0 and num < 100 and len(desc) > 3:\n",
    "                    if not any(x in desc.lower() for x in ['date', 'total', 'subtotal', 'payment', 'balance', 'qty', 'rate', 'amount']):\n",
    "                        descriptions.append({'num': num, 'desc': desc})\n",
    "        \n",
    "        # Match descriptions with prices\n",
    "        for i, desc_item in enumerate(descriptions):\n",
    "            if i < len(price_lines):\n",
    "                po_data['items'].append({\n",
    "                    'quantity': price_lines[i]['qty'],\n",
    "                    'description': desc_item['desc'],\n",
    "                    'price': price_lines[i]['rate'],\n",
    "                    'amount': price_lines[i]['amount']\n",
    "                })\n",
    "    \n",
    "    return po_data\n",
    "\n",
    "\n",
    "print(\"Parsing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7700d",
   "metadata": {},
   "source": [
    "## 5. Main Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "eb4bbea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "def process_single_image(image_path, preprocess_output_dir=None, use_preprocessing=True):\n",
    "    \"\"\"\n",
    "    Process a single purchase order image using Tesseract OCR.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the PO image\n",
    "        preprocess_output_dir: Optional folder to save preprocessed image\n",
    "        use_preprocessing: Whether to apply image preprocessing (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing extracted PO data\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {image_path}\")\n",
    "    \n",
    "    if use_preprocessing:\n",
    "        # Preprocess image once\n",
    "        processed_img = enhance_image(image_path)\n",
    "        \n",
    "        # Save preprocessed image if folder provided\n",
    "        if preprocess_output_dir:\n",
    "            os.makedirs(preprocess_output_dir, exist_ok=True)\n",
    "            base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            preprocessed_path = os.path.join(preprocess_output_dir, f\"{base_name}_preprocessed.png\")\n",
    "            cv2.imwrite(preprocessed_path, processed_img)\n",
    "        \n",
    "        # Extract text from preprocessed image\n",
    "        text = extract_text_from_image(image_path, preprocess=False, processed_img=processed_img)\n",
    "    else:\n",
    "        # Extract text directly without preprocessing\n",
    "        print(\"  (Using original image without preprocessing)\")\n",
    "        text = extract_text_from_image(image_path, preprocess=False)\n",
    "    \n",
    "    # Parse the extracted text\n",
    "    po_data = parse_purchase_order(text)\n",
    "    po_data['extraction_method'] = 'ocr'\n",
    "    \n",
    "    # Add filename to the data\n",
    "    po_data['source_file'] = os.path.basename(image_path)\n",
    "    \n",
    "    return po_data\n",
    "\n",
    "\n",
    "def process_multiple_images(image_folder, extensions=['*.png', '*.jpg', '*.jpeg', '*.tiff', '*.bmp', '*.webp', '*.gif'], preprocess_output_dir=None, use_preprocessing=True):\n",
    "    \"\"\"\n",
    "    Process multiple purchase order images from a folder using Tesseract OCR.\n",
    "    \n",
    "    Args:\n",
    "        image_folder: Path to the folder containing PO images\n",
    "        extensions: List of image file extensions to process\n",
    "        preprocess_output_dir: Optional folder to save preprocessed images\n",
    "        use_preprocessing: Whether to apply image preprocessing (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing extracted PO data\n",
    "    \"\"\"\n",
    "    all_po_data = []\n",
    "    \n",
    "    # Find all image files\n",
    "    image_files = []\n",
    "    for ext in extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(image_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(image_folder, ext.upper())))\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    image_files = sorted(list(set(image_files)))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process:\")\n",
    "    print(f\"Preprocessing: {'Enabled' if use_preprocessing else 'Disabled'}\")\n",
    "    for f in image_files:\n",
    "        print(f\"  - {f}\")\n",
    "    \n",
    "    for image_path in image_files:\n",
    "        try:\n",
    "            po_data = process_single_image(\n",
    "                image_path, \n",
    "                preprocess_output_dir=preprocess_output_dir,\n",
    "                use_preprocessing=use_preprocessing\n",
    "            )\n",
    "            all_po_data.append(po_data)\n",
    "            print(f\"  ✓ Successfully processed: {os.path.basename(image_path)}\")\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"  ✗ Error processing {image_path}:\")\n",
    "            print(f\"    {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return all_po_data\n",
    "\n",
    "\n",
    "print(\"Processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a14884",
   "metadata": {},
   "source": [
    "## 6. CSV Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ece36565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export functions defined!\n"
     ]
    }
   ],
   "source": [
    "def export_to_csv(po_data_list, output_path='purchase_orders.csv'):\n",
    "    \"\"\"\n",
    "    Export parsed purchase order data to CSV.\n",
    "    \n",
    "    Args:\n",
    "        po_data_list: List of dictionaries containing PO data\n",
    "        output_path: Path for the output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        Path to the created CSV file\n",
    "    \"\"\"\n",
    "    # Prepare data for CSV\n",
    "    csv_data = []\n",
    "    \n",
    "    for po in po_data_list:\n",
    "        # Get items info\n",
    "        items = po.get('items', [])\n",
    "        # Calculate total quantity from all items\n",
    "        total_quantity = sum(item.get('quantity', 0) for item in items) if items else 0\n",
    "        # Get all product descriptions joined\n",
    "        descriptions = [item.get('description', '') for item in items if item.get('description')]\n",
    "        product_desc = '; '.join(descriptions) if descriptions else ''\n",
    "        \n",
    "        # Create a row for the main PO data\n",
    "        row = {\n",
    "            'Source File': po.get('source_file', ''),\n",
    "            'PO Number': po.get('po_number', ''),\n",
    "            'Date': po.get('date', ''),\n",
    "            'Vendor Name': po.get('vendor_name', ''),\n",
    "            'Product Description': product_desc,\n",
    "            'Quantity': total_quantity,\n",
    "        }\n",
    "        csv_data.append(row)\n",
    "    \n",
    "    # Create DataFrame and export to CSV\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Exported {len(csv_data)} purchase orders to: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def export_detailed_csv(po_data_list, output_path='purchase_orders_detailed.csv'):\n",
    "    \"\"\"\n",
    "    Export detailed purchase order data including line items to CSV.\n",
    "    \n",
    "    Args:\n",
    "        po_data_list: List of dictionaries containing PO data\n",
    "        output_path: Path for the output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        Path to the created CSV file\n",
    "    \"\"\"\n",
    "    csv_data = []\n",
    "    \n",
    "    for po in po_data_list:\n",
    "        items = po.get('items', [])\n",
    "        \n",
    "        if items:\n",
    "            for item in items:\n",
    "                row = {\n",
    "                    'Source File': po.get('source_file', ''),\n",
    "                    'PO Number': po.get('po_number', ''),\n",
    "                    'Date': po.get('date', ''),\n",
    "                    'Total Amount': po.get('total_amount', ''),\n",
    "                    'Item Quantity': item.get('quantity', ''),\n",
    "                    'Item Description': item.get('description', ''),\n",
    "                    'Item Price': item.get('price', ''),\n",
    "                }\n",
    "                csv_data.append(row)\n",
    "        else:\n",
    "            # Add a row even if no items were detected\n",
    "            row = {\n",
    "                'Source File': po.get('source_file', ''),\n",
    "                'PO Number': po.get('po_number', ''),\n",
    "                'Date': po.get('date', ''),\n",
    "                'Total Amount': po.get('total_amount', ''),\n",
    "                'Item Quantity': '',\n",
    "                'Item Description': '',\n",
    "                'Item Price': '',\n",
    "            }\n",
    "            csv_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Exported detailed data to: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "print(\"Export functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbceb51",
   "metadata": {},
   "source": [
    "## 7. Visualization and Preview Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f71fc995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization functions defined!\n"
     ]
    }
   ],
   "source": [
    "def visualize_detection(image_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize text detection by drawing bounding boxes on the image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        output_path: Optional path to save the annotated image\n",
    "    \n",
    "    Returns:\n",
    "        Annotated image\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Get OCR data with bounding boxes\n",
    "    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "    \n",
    "    n_boxes = len(data['text'])\n",
    "    \n",
    "    for i in range(n_boxes):\n",
    "        if int(data['conf'][i]) > 60:  # Only draw boxes with confidence > 60%\n",
    "            (x, y, w, h) = (data['left'][i], data['top'][i], data['width'][i], data['height'][i])\n",
    "            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    if output_path:\n",
    "        cv2.imwrite(output_path, img)\n",
    "        print(f\"Annotated image saved to: {output_path}\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def preview_preprocessing(image_path):\n",
    "    \"\"\"\n",
    "    Preview the preprocessing steps on an image.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Original image\n",
    "    original = cv2.imread(image_path)\n",
    "    original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Preprocessed image\n",
    "    processed = enhance_image(image_path)\n",
    "    \n",
    "    # Display side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "    \n",
    "    axes[0].imshow(original_rgb)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(processed, cmap='gray')\n",
    "    axes[1].set_title('Preprocessed Image')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a9214",
   "metadata": {},
   "source": [
    "## 8. Run the Purchase Order Reader\n",
    "\n",
    "Configure the paths below and run to process your purchase order images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "894b8030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set!\n",
      "Image folder: d:\\Year 4 Sem 1\\PO\\PO\\images\n",
      "Output folder: d:\\Year 4 Sem 1\\PO\\PO\\output\n",
      "Preprocessed images folder: d:\\Year 4 Sem 1\\PO\\PO\\preprocessed_images\n",
      "Use preprocessing: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - Update these paths\n",
    "# ============================================\n",
    "\n",
    "# Get the notebook's directory as the base path\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "if not NOTEBOOK_DIR or NOTEBOOK_DIR == \".\":\n",
    "    NOTEBOOK_DIR = os.getcwd()\n",
    "\n",
    "# Option 1: Process a single image\n",
    "SINGLE_IMAGE_PATH = os.path.join(NOTEBOOK_DIR, \"images\", \"test1.png\")\n",
    "\n",
    "# Option 2: Process multiple images from a folder\n",
    "IMAGE_FOLDER = os.path.join(NOTEBOOK_DIR, \"images\")\n",
    "\n",
    "# Output folder and CSV file paths\n",
    "OUTPUT_FOLDER = os.path.join(NOTEBOOK_DIR, \"output\")\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_FOLDER, \"purchase_orders_output.csv\")\n",
    "OUTPUT_DETAILED_CSV = os.path.join(OUTPUT_FOLDER, \"purchase_orders_detailed.csv\")\n",
    "\n",
    "# Tesseract preprocessed images folder\n",
    "PREPROCESSED_FOLDER = os.path.join(NOTEBOOK_DIR, \"preprocessed_images\")\n",
    "\n",
    "# ============================================\n",
    "# PREPROCESSING TOGGLE - Set to False to skip preprocessing\n",
    "# ============================================\n",
    "USE_PREPROCESSING = True  # Set to True to enable preprocessing, False to use original images\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(PREPROCESSED_FOLDER, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration set!\")\n",
    "print(f\"Image folder: {IMAGE_FOLDER}\")\n",
    "print(f\"Output folder: {OUTPUT_FOLDER}\")\n",
    "print(f\"Preprocessed images folder: {PREPROCESSED_FOLDER}\")\n",
    "print(f\"Use preprocessing: {USE_PREPROCESSING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "08f4d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PROCESS SINGLE IMAGE (uncomment to use)\n",
    "# ============================================\n",
    "\n",
    "# if os.path.exists(SINGLE_IMAGE_PATH):\n",
    "#     # Process the image (also saves preprocessed image)\n",
    "#     po_data = process_single_image(SINGLE_IMAGE_PATH, preprocess_output_dir=PREPROCESSED_FOLDER)\n",
    "#     \n",
    "#     # Display extracted data\n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"EXTRACTED DATA:\")\n",
    "#     print(\"=\"*50)\n",
    "#     print(f\"PO Number: {po_data.get('po_number', 'Not found')}\")\n",
    "#     print(f\"Date: {po_data.get('date', 'Not found')}\")\n",
    "#     print(f\"Total Amount: {po_data.get('total_amount', 'Not found')}\")\n",
    "#     print(f\"Items found: {len(po_data.get('items', []))}\")\n",
    "#     \n",
    "#     # Export to CSV\n",
    "#     export_to_csv([po_data], OUTPUT_CSV)\n",
    "# else:\n",
    "#     print(f\"Image not found: {SINGLE_IMAGE_PATH}\")\n",
    "#     print(\"Please update SINGLE_IMAGE_PATH with a valid image path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "926aec70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images to process:\n",
      "Preprocessing: Enabled\n",
      "  - d:\\Year 4 Sem 1\\PO\\PO\\images\\originalPO.jpg\n",
      "  - d:\\Year 4 Sem 1\\PO\\PO\\images\\test.jpg\n",
      "Processing: d:\\Year 4 Sem 1\\PO\\PO\\images\\originalPO.jpg\n",
      "  ✓ Successfully processed: originalPO.jpg\n",
      "Processing: d:\\Year 4 Sem 1\\PO\\PO\\images\\test.jpg\n",
      "  ✓ Successfully processed: test.jpg\n",
      "\n",
      "==================================================\n",
      "PROCESSED 2 PURCHASE ORDERS\n",
      "==================================================\n",
      "\n",
      "1. originalPO.jpg\n",
      "   PO Number: SW02132230W\n",
      "   Date: 2026-01-26\n",
      "   Vendor: CRYSTAL MARTIN (HONG KONG)LTD\n",
      "   Total: $None\n",
      "\n",
      "2. test.jpg\n",
      "   PO Number: 4165\n",
      "   Date: None\n",
      "   Vendor: None\n",
      "   Total: $60000.0\n",
      "Exported 2 purchase orders to: d:\\Year 4 Sem 1\\PO\\PO\\output\\purchase_orders_output.csv\n",
      "Exported detailed data to: d:\\Year 4 Sem 1\\PO\\PO\\output\\purchase_orders_detailed.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PROCESS MULTIPLE IMAGES FROM FOLDER\n",
    "# ============================================\n",
    "\n",
    "if os.path.exists(IMAGE_FOLDER):\n",
    "    # Process all images in the folder using Tesseract OCR\n",
    "    all_po_data = process_multiple_images(\n",
    "        IMAGE_FOLDER, \n",
    "        preprocess_output_dir=PREPROCESSED_FOLDER if USE_PREPROCESSING else None,\n",
    "        use_preprocessing=USE_PREPROCESSING\n",
    "    )\n",
    "    \n",
    "    if all_po_data:\n",
    "        # Display summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"PROCESSED {len(all_po_data)} PURCHASE ORDERS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for i, po in enumerate(all_po_data, 1):\n",
    "            print(f\"\\n{i}. {po.get('source_file', 'Unknown')}\")\n",
    "            print(f\"   PO Number: {po.get('po_number', 'Not found')}\")\n",
    "            print(f\"   Date: {po.get('date', 'Not found')}\")\n",
    "            print(f\"   Vendor: {po.get('vendor_name', 'Not found')}\")\n",
    "            print(f\"   Total: ${po.get('total_amount', 'Not found')}\")\n",
    "        \n",
    "        # Export to CSV\n",
    "        export_to_csv(all_po_data, OUTPUT_CSV)\n",
    "        export_detailed_csv(all_po_data, OUTPUT_DETAILED_CSV)\n",
    "    else:\n",
    "        print(\"No images found or processed.\")\n",
    "else:\n",
    "    print(f\"Folder not found: {IMAGE_FOLDER}\")\n",
    "    print(\"Please create the folder and add PO images, or update IMAGE_FOLDER path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9aeee179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ITEMS EXTRACTED ===\n",
      "\n",
      "File: originalPO.jpg\n",
      "  Items found: 1\n",
      "    - Qty: 1319, Desc: VS Panty Global Heat Transfer C/70 - Cold Peel (Angel Pink), Rate: $0, Amount: $0\n",
      "\n",
      "File: test.jpg\n",
      "  Items found: 0\n"
     ]
    }
   ],
   "source": [
    "# Check items extracted from each PO\n",
    "print(\"=== ITEMS EXTRACTED ===\")\n",
    "for po in all_po_data:\n",
    "    print(f\"\\nFile: {po.get('source_file')}\")\n",
    "    print(f\"  Items found: {len(po.get('items', []))}\")\n",
    "    for item in po.get('items', []):\n",
    "        print(f\"    - Qty: {item.get('quantity')}, Desc: {item.get('description')}, Rate: ${item.get('price')}, Amount: ${item.get('amount', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c5052",
   "metadata": {},
   "source": [
    "## 9. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e82b3ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purchase Orders Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source File</th>\n",
       "      <th>PO Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Vendor Name</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>originalPO.jpg</td>\n",
       "      <td>SW02132230W</td>\n",
       "      <td>2026-01-26</td>\n",
       "      <td>CRYSTAL MARTIN (HONG KONG)LTD</td>\n",
       "      <td>VS Panty Global Heat Transfer C/70 - Cold Peel...</td>\n",
       "      <td>1319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test.jpg</td>\n",
       "      <td>4165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Source File    PO Number        Date                    Vendor Name  \\\n",
       "0  originalPO.jpg  SW02132230W  2026-01-26  CRYSTAL MARTIN (HONG KONG)LTD   \n",
       "1        test.jpg         4165         NaN                            NaN   \n",
       "\n",
       "                                 Product Description  Quantity  \n",
       "0  VS Panty Global Heat Transfer C/70 - Cold Peel...      1319  \n",
       "1                                                NaN         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the exported CSV file\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    df = pd.read_csv(OUTPUT_CSV)\n",
    "    print(\"Purchase Orders Summary:\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(f\"CSV file not found: {OUTPUT_CSV}\")\n",
    "    print(\"Please run the processing cells above first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a3900277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Purchase Order Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source File</th>\n",
       "      <th>PO Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Total Amount</th>\n",
       "      <th>Item Quantity</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Item Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>originalPO.jpg</td>\n",
       "      <td>SW02132230W</td>\n",
       "      <td>2026-01-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1319.0</td>\n",
       "      <td>VS Panty Global Heat Transfer C/70 - Cold Peel...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test.jpg</td>\n",
       "      <td>4165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Source File    PO Number        Date  Total Amount  Item Quantity  \\\n",
       "0  originalPO.jpg  SW02132230W  2026-01-26           NaN         1319.0   \n",
       "1        test.jpg         4165         NaN       60000.0            NaN   \n",
       "\n",
       "                                    Item Description  Item Price  \n",
       "0  VS Panty Global Heat Transfer C/70 - Cold Peel...         0.0  \n",
       "1                                                NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View detailed CSV with line items\n",
    "if os.path.exists(OUTPUT_DETAILED_CSV):\n",
    "    df_detailed = pd.read_csv(OUTPUT_DETAILED_CSV)\n",
    "    print(\"Detailed Purchase Order Data:\")\n",
    "    display(df_detailed)\n",
    "else:\n",
    "    print(f\"Detailed CSV file not found: {OUTPUT_DETAILED_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6226a634",
   "metadata": {},
   "source": [
    "## 10. Debug and Test OCR on a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3fd9986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: View raw OCR output for a single image\n",
    "# Uncomment and update the path to test\n",
    "\n",
    "# TEST_IMAGE = r\"C:\\PO\\test_image.png\"\n",
    "# \n",
    "# if os.path.exists(TEST_IMAGE):\n",
    "#     # Preview preprocessing\n",
    "#     preview_preprocessing(TEST_IMAGE)\n",
    "#     \n",
    "#     # Extract and display raw text\n",
    "#     raw_text = extract_text_from_image(TEST_IMAGE)\n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"RAW OCR OUTPUT:\")\n",
    "#     print(\"=\"*50)\n",
    "#     print(raw_text)\n",
    "# else:\n",
    "#     print(f\"Test image not found: {TEST_IMAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "769fa57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini models:\n",
      "  - models/gemini-2.5-flash\n",
      "  - models/gemini-2.5-pro\n",
      "  - models/gemini-2.0-flash\n",
      "  - models/gemini-2.0-flash-001\n",
      "  - models/gemini-2.0-flash-exp-image-generation\n",
      "  - models/gemini-2.0-flash-lite-001\n",
      "  - models/gemini-2.0-flash-lite\n",
      "  - models/gemini-exp-1206\n",
      "  - models/gemini-2.5-flash-preview-tts\n",
      "  - models/gemini-2.5-pro-preview-tts\n",
      "  - models/gemini-flash-latest\n",
      "  - models/gemini-flash-lite-latest\n",
      "  - models/gemini-pro-latest\n",
      "  - models/gemini-2.5-flash-lite\n",
      "  - models/gemini-2.5-flash-image\n",
      "  - models/gemini-2.5-flash-preview-09-2025\n",
      "  - models/gemini-2.5-flash-lite-preview-09-2025\n",
      "  - models/gemini-3-pro-preview\n",
      "  - models/gemini-3-flash-preview\n",
      "  - models/gemini-3-pro-image-preview\n",
      "  - models/gemini-robotics-er-1.5-preview\n",
      "  - models/gemini-2.5-computer-use-preview-10-2025\n",
      "  - models/gemini-embedding-001\n",
      "  - models/gemini-2.5-flash-native-audio-latest\n",
      "  - models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "  - models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "# List available Gemini models\n",
    "if gemini_client:\n",
    "    print(\"Available Gemini models:\")\n",
    "    for model in gemini_client.models.list():\n",
    "        if 'gemini' in model.name.lower():\n",
    "            print(f\"  - {model.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
